package org.deeplearning4j.optimize.solvers.accumulation;

import lombok.NonNull;
import org.nd4j.linalg.api.memory.MemoryWorkspace;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.List;
import java.util.Queue;
import java.util.concurrent.LinkedTransferQueue;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

/**
 * This class provides accumulation for gradients for both input (i.e. updates coming from network) and output (comint from one ore more models training at the same time)
 *
 * @author raver119@gmail.com
 */
public class GradientsAccumulator implements Serializable {

    protected MessageHandler handler;

    // here we'll store messages coming from "somewhere else"
    protected transient Queue<INDArray> gradients;

    // this field stores current accumulated
    protected transient INDArray storage;

    // this counter tracks number of messages generated by this accumulation
    protected transient AtomicLong ownCounter = new AtomicLong(0);

    // this counter tracks number of messages received from somewhere
    protected transient AtomicLong extCounter = new AtomicLong(0);

    // FIXME: this mechanics should be improved i think.
    protected int[] shape;
    protected char ordering;

    protected List<INDArray> consumerData = new ArrayList<>();
    protected List<Object> consumerLocks = new ArrayList<>();
    protected ThreadLocal<Integer> consumerIndex = new ThreadLocal<>();
    protected AtomicInteger consumers = new AtomicInteger(0);

    /**
     * Creates new GradientsAccumulator with starting threshold of 1e-3
     */
    public GradientsAccumulator() {
        this(new LocalHandler());
    }

    /**
     * Creates new GradientsAccumulator with custom starting threshold
     *
     * @param handler MessageHandler instance that'll be used for communication purposes
     */
    public GradientsAccumulator(@NonNull MessageHandler handler) {
        this.gradients = new LinkedTransferQueue<>();
        this.handler = handler;

        this.handler.initialize(this);
    }

    // FIXME: this implementation is wrong, and might cause rc
    public INDArray getUpdate() {
        synchronized (this) {
            if (consumerIndex.get() == null) {
                try (MemoryWorkspace workspace = Nd4j.getMemoryManager().scopeOutOfWorkspaces()) {
                    consumerData.add(Nd4j.create(shape, ordering));
                    consumerLocks.add(new Object());
                    consumerIndex.set(consumers.getAndIncrement());
                }
            }
        }

        synchronized (consumerLocks.get(consumerIndex.get())) {
            // this code should run within workspace

            INDArray updates = consumerData.get(consumerIndex.get());
            INDArray ret = updates.dup(ordering);

            Nd4j.getExecutioner().commit();

            // we assign to 0.0, so all future incoming updates will be able to call for addi here
            updates.assign(0.0);

            return ret;
        }
    }

    /**
     * This method accepts updates suitable for StepFunction, and accumulates/propagates it across all workers
     *
     * @param array
     */
    // TODO: this method should be synchronized probably, if we want to call this method from different threads
    public synchronized void storeUpdate(INDArray array) {
        /*
            Here we want to do 4 things:
            1) update accumulated values
            2) invoke extractor, that'll (optionally) pull all updates above threshold
            3) ???
            4) PROFIT!
         */

        // if accum is null, let's just create it here
        if (storage == null) {
            // we don't want state array to be attached to any workspace
            shape = array.shape();
            ordering = array.ordering();

            try (MemoryWorkspace workspace = Nd4j.getMemoryManager().scopeOutOfWorkspaces()) {
                storage = Nd4j.create(shape, ordering);
            }
        }

        // accumulate our values, a
        storage.addi(array);

        // we ensure storage was updated successfully
        Nd4j.getExecutioner().commit();

        // if there's something to send - send it. Skip otherwise!!!
        if (handler.broadcastUpdates(storage)) {
            ownCounter.getAndIncrement();
        }
    }


    /**
     * This method accepts updates suitable for StepFunction and puts them to the queue, which is used in backpropagation loop
     *
     * @param array
     */
    public void receiveUpdate(INDArray array) {
        extCounter.getAndIncrement();

        // FIXME: something better needed here
        for (int i = 0; i < consumerData.size(); i++) {
            synchronized (consumerLocks.get(i)) {
                consumerData.get(i).addi(array);
            }
        }

        // we have to ensure, all operations were finished here
        Nd4j.getExecutioner().commit();
    }


    /**
     * This method resets all accumulated updates (if any)
     */
    public void reset() {
        if (storage != null)
            storage.assign(0.0f);
    }
}
