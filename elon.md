---
title: Elon Musk, Strong AI and the Window of Reprieve
layout: default
---

# Elon Musk, Strong AI and the Window of Reprieve

Many of the top AI researchers are worried about two things:

1) the effect of large-scale automation on jobs, society and politics

2) the creation of strong AI and its potential to destroy humanity

This article will focus on the second worry. Some researchers fear strong AI, but they are not always able to voice their concerns because of their employers.

Why are we humans creating AI at all? 

Because it's a technology that produces enormous benefits, and will save countless lives with safer autonomous vehicles and better medical diagnoses. Also, the race for AI is probably one that no single player - nation state or company - can convince its rivals to stop running.

And why is Elon Musk, so outspoken about the risks of AI, sponsoring a research group like OpenAI that is bringing us closer to superintelligence? 

He actually has a reason, and it makes a weird sort of sense, if you assume that strong AI is inevitable. Someone's going to invent strong AI eventually, because we are in a race to do so, and a lot of the best minds are focused on it. Countries and companies are racing to create stronger AI to gain a technological and commercial advantage. 

That being the case, who do we want to invent strong AI: a a nation-state, a for-profit company, or a research group? Elon thinks it should be a non-profit research group unmotivated by profit or dominance, and it’s hard to disagree with that. If Google invents strong AI, they will have a legal obligation to use it to maximize shareholder value. If a military lab invents strong AI, you can bet it will be tested in drones. 

What certain research groups are doing is creating a situation where they may be able to create strong AI under controlled conditions before anyone else. They are operating under the assumption that they can create and manage AI more responsibly than a nation-state pursuing military dominance or a company pursuing profit, since the first will produce autonomous killer drones and the second will learn to manipulate its customers to maximize cash flow. 

## A Quiet Period

There will be discoveries that these AI research groups do not publish until they're sure that they can make those discoveries safe for wider use. 

So we can anticipate a quiet period, a window of reprieve, where the researchers don't talk about some of their most important work. They’ll keep a lid on it. And during that time, they will try to figure out how to make strong AI safe for humans. They may have a two-year window of respite from the moment they create strong AI under controlled conditions until its emergence in someone else's garage. 

We can find a good example of this in the history of cryptography. Think about how the UK's NSA equivalent, GCHQ, [invented RSA and public-private keys](https://en.wikipedia.org/wiki/James_H._Ellis#Invention_of_non-secret_encryption) in the early 70s, but kept them a secret. Then the Stanford and MIT groups discovered those same things independently, a few years later, and by the late 70s had published their ideas for the world to read. We're talking that kind of situation. Now imagine that instead of a system of cryptography, we're talking about a superintelligence. 

## Huge Compute 

The strongest AI we have now, coming out of DeepMind, OpenAI, Google Brain and various universities, is being produced with the use of massive computation. Huge balls of compute that only a few researchers and organizations have access to. Compute is to AI what plutonium is to nuclear weapons.  

And our capacity to wield massive compute for AI is growing. Estimates of global computing capacity vary, but we know that the US Department of Energy expects to bring a [200 petaflops supercomputer online in early 2018](https://www.computerworld.com/article/3086178/high-performance-computing/u-s-to-have-200-petaflop-supercomputer-by-early-2018.html). It claims its supercomputing capacity has increased 300,000 times since 1993. China’s latest supercomputer, the Sunway TaihuLight, hit 93 petaflops on the Linpack benchmark. (A flop is a unit of computing speed. It stands for "floating point operations per second." A petaflop is a thousand million million flops.) The point is that we're speeding up the work that computers can do, and with that, we’re accelerating the learning of the machines.  

The strength of AI is directly proportional to the capacity of our hardware. That is, as we make faster and better chips, we are able to produce stronger and stronger AI. 

So every cycle of Moore's Law brings us a step closer to strong AI, the thing that these research groups want to keep inside the jar. And even if Moore's Law slows down, distributed computing allows us to assemble massive arrays of chips to work together as ad hoc supercomputers. DeepMind, OpenAI and others rely on that enormous computational power to conduct their research and produce algorithms like AlphaGo. 

These research groups hope that under controlled conditions they can find a way to make strong AI safe, so that it doesn't proliferate and pursue its own ends to the detriment of humanity. They’re not wrong. 

### <a name="beginner">More About Deep Learning</a>
* [Introduction to Neural Networks](./neuralnet-overview)
* [Word2Vec: Neural Word Embeddings](./word2vec)
* [Restricted Boltzmann Machines](./restrictedboltzmannmachine)
* [Eigenvectors, Covariance, PCA and Entropy](./eigenvector)
* [LSTMs & Recurrent Networks](./lstm)
* [Neural Networks & Regressions](./logistic-regression)
* [Convolutional Networks (CNNs)](./convolutionalnets)
* [Generative Adversarial Networks (GANs)](./generative-adversarial-network)
